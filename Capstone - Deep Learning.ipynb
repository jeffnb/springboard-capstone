{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.8.0-py2.py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 10.2 MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (1.18.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (1.14.0)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow_hub) (3.11.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow_hub) (46.0.0)\n",
      "Installing collected packages: tensorflow-hub\n",
      "Successfully installed tensorflow-hub-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tqdm\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 28 13:21:13 2020       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 418.67       Driver Version: 418.67       CUDA Version: 10.1     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Quadro P4000        On   | 00000000:00:05.0 Off |                  N/A |\r\n",
      "| 46%   26C    P8     5W / 105W |      1MiB /  8119MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIRECTORY = 'datasets/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load up data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First up corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_PREPROCESSED_CORPUS_DUMP = \"dl_preprocessed_corpus.p\"\n",
    "corpus = pickle.load(open(os.path.join(DATA_DIRECTORY, DL_PREPROCESSED_CORPUS_DUMP), \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the tokenized tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DL_TOKENIZED_DUMP = \"dl_tokenized_tweets.p\"\n",
    "tokenized = pickle.load(open(os.path.join(DATA_DIRECTORY, DL_TOKENIZED_DUMP), \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load up the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DUMP_FILE = \"combined_data.p\"\n",
    "data = pickle.load(open(os.path.join(DATA_DIRECTORY, DUMP_FILE), \"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['cleaned'] = corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class</th>\n",
       "      <th>tweet</th>\n",
       "      <th>id</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>!!! RT @mayasolovely: As a woman you shouldn't...</td>\n",
       "      <td>0</td>\n",
       "      <td>as a woman you should not complain about clean...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!! RT @mleew17: boy dats cold...tyga dwn ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>boi dat cold tyga dwn bad for cuffin dat hoe i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...</td>\n",
       "      <td>2</td>\n",
       "      <td>dawg sbabylif you ever fuck a bitch and she st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!! RT @C_G_Anderson: @viva_based she lo...</td>\n",
       "      <td>3</td>\n",
       "      <td>she look like a tranni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>!!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...</td>\n",
       "      <td>4</td>\n",
       "      <td>the shit you hear about me might be true or it...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   class                                              tweet  id  \\\n",
       "0      2  !!! RT @mayasolovely: As a woman you shouldn't...   0   \n",
       "1      1  !!!!! RT @mleew17: boy dats cold...tyga dwn ba...   1   \n",
       "2      1  !!!!!!! RT @UrKindOfBrand Dawg!!!! RT @80sbaby...   2   \n",
       "3      1  !!!!!!!!! RT @C_G_Anderson: @viva_based she lo...   3   \n",
       "4      1  !!!!!!!!!!!!! RT @ShenikaRoberts: The shit you...   4   \n",
       "\n",
       "                                             cleaned  \n",
       "0  as a woman you should not complain about clean...  \n",
       "1  boi dat cold tyga dwn bad for cuffin dat hoe i...  \n",
       "2  dawg sbabylif you ever fuck a bitch and she st...  \n",
       "3                             she look like a tranni  \n",
       "4  the shit you hear about me might be true or it...  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to lists\n",
    "\n",
    "It was found that keeping things as a series causes strangeness since series index with an index but the lists are ordered indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = data.cleaned.values\n",
    "classes = data['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['as a woman you should not complain about clean up your hous as a man you should alwai take the trash out',\n",
       "       'boi dat cold tyga dwn bad for cuffin dat hoe in the st place',\n",
       "       'dawg sbabylif you ever fuck a bitch and she sta to cry you be confus as shit'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 1])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(tweets, classes, test_size=0.2, \n",
    "                                                    stratify=classes, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Most of this flow was adapted from the [Deep TL hack session by DJ Sarkar](https://github.com/dipanjanS/deep_transfer_learning_nlp_dhs2019/blob/master/notebooks/Deep%20TL%20for%20NLP%20-%20Demo.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start by creating a vocabulary\n",
    "This creates a tokenizer and fits it to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
    "t.fit_on_texts(X_train)\n",
    "t.word_index['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('heimer', 33755) ('<PAD>', 0) 1\n"
     ]
    }
   ],
   "source": [
    "print(max([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), \n",
    "      min([(k, v) for k, v in t.word_index.items()], key = lambda x:x[1]), \n",
    "      t.word_index['<UNK>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to numbers\n",
    "\n",
    "We need to convert the text to numbers in the vocabulary index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq = t.texts_to_sequences(X_train)\n",
    "X_test_seq = t.texts_to_sequences(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('closet muzzi', [2153, 656])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0], X_train_seq[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'closet muzzi'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.sequences_to_texts(X_train_seq)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size=33756\n",
      "Number of Documents=35698\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary size={}\".format(len(t.word_index)))\n",
    "print(\"Number of Documents={}\".format(t.document_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAPQUlEQVR4nO3df6zddX3H8efL4q/pYkFuCGnJbhebmZpMIA1iNIuDCJUa4Q81GDMb06T/dAkmJq5syYg/SMo/oibThAizGmNl6EYDRtYVzLI/BIogUjrGVWtoA7bagnNGsuJ7f5xPzVm9t/dcen+d+3k+kpP7/X6+33PO+6tfXufTz/l8vydVhSSpD69Y6gIkSYvH0Jekjhj6ktQRQ1+SOmLoS1JHzlnqAs7k/PPPr8nJyaUuQ5LGyiOPPPKLqpqYbtuyDv3JyUn279+/1GVI0lhJ8rOZtjm8I0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHVnWV+Rq4UzuuHek/Q7t3LzAlUhaTPb0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktSRkUM/yaokjya5p62vS/Jgkqkk30zyqtb+6rY+1bZPDr3Gja39qSRXz/fBSJLObC49/RuAg0PrtwC3VtWbgBPA1ta+FTjR2m9t+5FkA3A98BZgE/DFJKvOrnxJ0lyMFPpJ1gKbgS+39QBXAHe1XXYB17Xla9s6bfuVbf9rgd1V9WJV/RSYAi6bj4OQJI1m1J7+54BPAL9r628Enq+qk239MLCmLa8BngFo219o+/++fZrnSJIWwayhn+S9wNGqemQR6iHJtiT7k+w/duzYYrylJHVjlJ7+O4D3JTkE7GYwrPN5YHWSU7+8tRY40paPABcBtO1vAH453D7Nc36vqm6rqo1VtXFiYmLOByRJmtmsoV9VN1bV2qqaZPBF7P1V9WHgAeD9bbctwN1teU9bp22/v6qqtV/fZvesA9YDD83bkUiSZnU2v5H7N8DuJJ8BHgVub+23A19LMgUcZ/BBQVUdSHIn8CRwEtheVS+dxftLkuZoTqFfVd8DvteWf8I0s2+q6rfAB2Z4/s3AzXMtUpI0P7wiV5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkbP55SzNYHLHvSPtd2jn5gWuRJL+P3v6ktQRe/pLyH8RSFps9vQlqSOGviR1xOGdMeAwkKT5Yk9fkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcQpmyvIqFM7JfXLnr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXEK3LnwCteJY27WXv6SV6T5KEkP0xyIMknW/u6JA8mmUryzSSvau2vbutTbfvk0Gvd2NqfSnL1Qh2UJGl6owzvvAhcUVVvBS4GNiW5HLgFuLWq3gScALa2/bcCJ1r7rW0/kmwArgfeAmwCvphk1XwejCTpzGYN/Rr4dVt9ZXsUcAVwV2vfBVzXlq9t67TtVyZJa99dVS9W1U+BKeCyeTkKSdJIRvoiN8mqJI8BR4G9wI+B56vqZNvlMLCmLa8BngFo218A3jjcPs1zht9rW5L9SfYfO3Zs7kckSZrRSKFfVS9V1cXAWga98zcvVEFVdVtVbayqjRMTEwv1NpLUpTlN2ayq54EHgLcDq5Ocmv2zFjjSlo8AFwG07W8AfjncPs1zJEmLYJTZOxNJVrfl1wLvBg4yCP/3t922AHe35T1tnbb9/qqq1n59m92zDlgPPDRfByJJmt0o8/QvBHa1mTavAO6sqnuSPAnsTvIZ4FHg9rb/7cDXkkwBxxnM2KGqDiS5E3gSOAlsr6qX5vdwJElnMmvoV9XjwCXTtP+EaWbfVNVvgQ/M8Fo3AzfPvUxJ0nzwNgyS1BFDX5I6YuhLUkcMfUnqiKEvSR3x1so6o1FvJ31o5+YFrkTSfLCnL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUke8n77mhffdl8aDPX1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSR7w4S4vKi7ikpWVPX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXEKZuMPo1QksadPX1J6sisoZ/koiQPJHkyyYEkN7T285LsTfJ0+3tua0+SLySZSvJ4kkuHXmtL2//pJFsW7rAkSdMZpad/Evh4VW0ALge2J9kA7AD2VdV6YF9bB3gPsL49tgFfgsGHBHAT8DbgMuCmUx8UkqTFMWvoV9WzVfWDtvzfwEFgDXAtsKvttgu4ri1fC3y1Br4PrE5yIXA1sLeqjlfVCWAvsGlej0aSdEZzGtNPMglcAjwIXFBVz7ZNzwEXtOU1wDNDTzvc2mZqP/09tiXZn2T/sWPH5lKeJGkWI4d+ktcD3wI+VlW/Gt5WVQXUfBRUVbdV1caq2jgxMTEfLylJakYK/SSvZBD4X6+qb7fmn7dhG9rfo639CHDR0NPXtraZ2iVJi2SU2TsBbgcOVtVnhzbtAU7NwNkC3D3U/pE2i+dy4IU2DHQfcFWSc9sXuFe1NknSIhnl4qx3AH8F/CjJY63tb4GdwJ1JtgI/Az7Ytn0HuAaYAn4DfBSgqo4n+TTwcNvvU1V1fF6OQpI0kllDv6r+A8gMm6+cZv8Cts/wWncAd8ylQEnS/PGKXEnqiKEvSR0x9CWpI4a+JHXE0Jekjng/fS1Lo/7GwaGdmxe4EmllsacvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI+csdQHS2Zjcce9I+x3auXmBK5HGgz19SeqIoS9JHTH0Jakjhr4kdcTQl6SOzBr6Se5IcjTJE0Nt5yXZm+Tp9vfc1p4kX0gyleTxJJcOPWdL2//pJFsW5nAkSWcySk//K8Cm09p2APuqaj2wr60DvAdY3x7bgC/B4EMCuAl4G3AZcNOpDwpJ0uKZNfSr6t+B46c1Xwvsasu7gOuG2r9aA98HVie5ELga2FtVx6vqBLCXP/wgkSQtsJc7pn9BVT3blp8DLmjLa4BnhvY73Npmav8DSbYl2Z9k/7Fjx15meZKk6Zz1F7lVVUDNQy2nXu+2qtpYVRsnJibm62UlSbz80P95G7ah/T3a2o8AFw3tt7a1zdQuSVpELzf09wCnZuBsAe4eav9Im8VzOfBCGwa6D7gqybntC9yrWpskaRHNesO1JN8A3gWcn+Qwg1k4O4E7k2wFfgZ8sO3+HeAaYAr4DfBRgKo6nuTTwMNtv09V1elfDkuSFtisoV9VH5ph05XT7FvA9hle5w7gjjlVJ0maV16RK0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHZn1itxxNrnj3qUuQZKWFXv6ktSRFd3Tl06Zy7/6Du3cvICVSEvLnr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEX8jVzrNqL+n62/pahzZ05ekjhj6ktQRQ1+SOmLoS1JHDH1J6oizd6SXyVk+Gkf29CWpI4se+kk2JXkqyVSSHYv9/pLUs0Ud3kmyCvgH4N3AYeDhJHuq6snFrENajkYdLhrVUg4rOfS1fC32mP5lwFRV/QQgyW7gWsDQ14o132G+3N93LsahxlGNywfYYof+GuCZofXDwNuGd0iyDdjWVn+d5KmzeL/zgV+cxfOX0jjXDuNd/zjXDuNd/9jWnluA5VP/n8y0YdnN3qmq24Db5uO1kuyvqo3z8VqLbZxrh/Guf5xrh/Guf5xrh/Gof7G/yD0CXDS0vra1SZIWwWKH/sPA+iTrkrwKuB7Ys8g1SFK3FnV4p6pOJvlr4D5gFXBHVR1YwLecl2GiJTLOtcN41z/OtcN41z/OtcMY1J+qWuoaJEmLxCtyJakjhr4kdWRFhv643eohyR1JjiZ5YqjtvCR7kzzd/p67lDXOJMlFSR5I8mSSA0luaO3jUv9rkjyU5Iet/k+29nVJHmzn0DfbxINlKcmqJI8muaetj1Pth5L8KMljSfa3tnE5d1YnuSvJfyY5mOTt41D7igv9oVs9vAfYAHwoyYalrWpWXwE2nda2A9hXVeuBfW19OToJfLyqNgCXA9vb/97jUv+LwBVV9VbgYmBTksuBW4Bbq+pNwAlg6xLWOJsbgIND6+NUO8BfVtXFQ/Pbx+Xc+Tzw3ap6M/BWBv8fLP/aq2pFPYC3A/cNrd8I3LjUdY1Q9yTwxND6U8CFbflC4KmlrnHE47ibwb2Vxq5+4I+AHzC4SvwXwDnTnVPL6cHgWpd9wBXAPUDGpfZW3yHg/NPalv25A7wB+CltMsw41b7ievpMf6uHNUtUy9m4oKqebcvPARcsZTGjSDIJXAI8yBjV34ZHHgOOAnuBHwPPV9XJtstyPoc+B3wC+F1bfyPjUztAAf+a5JF2CxYYj3NnHXAM+Mc2tPblJK9jDGpfiaG/4tSg27Cs59YmeT3wLeBjVfWr4W3Lvf6qeqmqLmbQa74MePMSlzSSJO8FjlbVI0tdy1l4Z1VdymA4dnuSvxjeuIzPnXOAS4EvVdUlwP9w2lDOcq19JYb+SrnVw8+TXAjQ/h5d4npmlOSVDAL/61X17dY8NvWfUlXPAw8wGBJZneTUxYvL9Rx6B/C+JIeA3QyGeD7PeNQOQFUdaX+PAv/M4EN3HM6dw8Dhqnqwrd/F4ENg2de+EkN/pdzqYQ+wpS1vYTBWvuwkCXA7cLCqPju0aVzqn0iyui2/lsH3EQcZhP/7227Lsv6qurGq1lbVJIPz/P6q+jBjUDtAktcl+eNTy8BVwBOMwblTVc8BzyT5s9Z0JYNbxC/72pf8S4UF+pLlGuC/GIzN/t1S1zNCvd8AngX+l0EPYiuDsdl9wNPAvwHnLXWdM9T+Tgb/hH0ceKw9rhmj+v8ceLTV/wTw9639T4GHgCngn4BXL3WtsxzHu4B7xqn2VucP2+PAqf9Wx+jcuRjY386dfwHOHYfavQ2DJHVkJQ7vSJJmYOhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0JekjvwfjRy3+zGYsrwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(doc.split()) for doc in X_train], bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pad the text with the max of 34\n",
    "\n",
    "Using 34 since it seems to cover almost every case in the histogram.  This will pad the smaller ones with padding variables so everything is an equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 34\n",
    "\n",
    "X_train = pad_sequences(X_train_seq, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "X_test = pad_sequences(X_test_seq, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((35698, 34), (8925, 34))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Twitter FastText Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "FASTTEXT_FILE = './datasets/glove.twitter.27B.200d.txt'\n",
    "VOCAB_SIZE = len(t.word_index)\n",
    "EMBED_SIZE = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33756"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index = t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext_embeddings(embedding_file, embedding_size):\n",
    "    \"\"\"\n",
    "    Simply opens up the file and creates a dictionary of word to embeddings.\n",
    "    This is almost directly taken from the article listed above\n",
    "    embedding_file: path to fasttest file with vectors\n",
    "    embedding_size: the number of dimensions of the vectors\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    with open(embedding_file, encoding=\"utf8\", errors='ignore') as f:\n",
    "        word_vectors = dict(get_coefs(*row.split(' ')) for row in f.readlines())\n",
    "        \n",
    "    return word_vectors\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings = load_fasttext_embeddings(FASTTEXT_FILE, EMBED_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_embeddings(word_to_index, max_features, word_embeddings):\n",
    "    \"\"\"\n",
    "    Short version of what will be done:\n",
    "    1. Get the mean and std of the embeddings.\n",
    "    2. create a matrix with random values from the norm over the entire matrix\n",
    "    3. run through the list of indexed words and pull any fasttext matches\n",
    "    \n",
    "    word_to_index: the dict that connectst he words to the index number that replaces them\n",
    "    max_features: vocabulary size\n",
    "    word_embeddings: dict of words mapped to their embeddings\n",
    "    \"\"\"\n",
    "    # Set up information about the embeddings to be able to set defaults\n",
    "    word_vectors = np.stack(word_embeddings.values())\n",
    "    embed_mean = word_vectors.mean()\n",
    "    embed_std = word_vectors.std()\n",
    "    embed_size = word_vectors.shape[1]\n",
    "    \n",
    "    # Generate the randomized matrix within norms\n",
    "    max_vocab = min(max_features, len(word_to_index))\n",
    "    embedding_matrix = np.random.normal(embed_mean, embed_std, (max_vocab, embed_size))\n",
    "    \n",
    "    # Loop through the word to index and overlay known embeddings over the random\n",
    "    for word, idx in word_to_index.items():\n",
    "        if idx >= max_vocab:\n",
    "            continue\n",
    "        embedding_vector = word_embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[idx] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3331: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "feature_embed = load_pretrained_embeddings(word2index, VOCAB_SIZE, word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33756, 200)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35698, 34)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33756"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOCAB_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need to change y for multi class\n",
    "\n",
    "Since we are doing multiclass classification we need to one hot encode the y_test and y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hot = tf.keras.utils.to_categorical(y_train)\n",
    "y_test_hot = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0.], dtype=float32), 0)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_hot[1000], y_train[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=100\n",
    "BATCH_SIZE=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, Dropout, Attention, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_37 (Embedding)     (None, 34, 200)           6751200   \n",
      "_________________________________________________________________\n",
      "bidirectional_32 (Bidirectio (None, 34, 512)           935936    \n",
      "_________________________________________________________________\n",
      "lstm_58 (LSTM)               (None, 256)               787456    \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_64 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_65 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 8,606,947\n",
      "Trainable params: 8,606,947\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=VOCAB_SIZE, output_dim=EMBED_SIZE, input_length=MAX_SEQUENCE_LENGTH, \n",
    "#                     batch_input_shape=(BATCH_SIZE, MAX_SEQUENCE_LENGTH),\n",
    "#                     batch_size=BATCH_SIZE,\n",
    "                    weights=[feature_embed], trainable=True))\n",
    "model.add(Bidirectional(LSTM(units=256, return_sequences=True)))\n",
    "model.add(LSTM(units=256, return_sequences=False))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy', 'accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                             patience=3,\n",
    "                                             restore_best_weights=True,\n",
    "                                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35698 samples, validate on 8925 samples\n",
      "Epoch 1/100\n",
      "35698/35698 [==============================] - 22s 611us/sample - loss: 0.4399 - categorical_accuracy: 0.8293 - accuracy: 0.8293 - val_loss: 0.3303 - val_categorical_accuracy: 0.8732 - val_accuracy: 0.8732\n",
      "Epoch 2/100\n",
      "35698/35698 [==============================] - 17s 479us/sample - loss: 0.3022 - categorical_accuracy: 0.8832 - accuracy: 0.8832 - val_loss: 0.3079 - val_categorical_accuracy: 0.8836 - val_accuracy: 0.8836\n",
      "Epoch 3/100\n",
      "35698/35698 [==============================] - 17s 467us/sample - loss: 0.2266 - categorical_accuracy: 0.9115 - accuracy: 0.9115 - val_loss: 0.3251 - val_categorical_accuracy: 0.8845 - val_accuracy: 0.8845\n",
      "Epoch 4/100\n",
      "35698/35698 [==============================] - 17s 465us/sample - loss: 0.1499 - categorical_accuracy: 0.9457 - accuracy: 0.9457 - val_loss: 0.3337 - val_categorical_accuracy: 0.8841 - val_accuracy: 0.8841\n",
      "Epoch 5/100\n",
      "35456/35698 [============================>.] - ETA: 0s - loss: 0.0977 - categorical_accuracy: 0.9646 - accuracy: 0.9646Restoring model weights from the end of the best epoch.\n",
      "35698/35698 [==============================] - 17s 490us/sample - loss: 0.0980 - categorical_accuracy: 0.9644 - accuracy: 0.9644 - val_loss: 0.4104 - val_categorical_accuracy: 0.8820 - val_accuracy: 0.8820\n",
      "Epoch 00005: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f29e4a4e588>"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train_hot, validation_data=[X_test, y_test_hot], callbacks=[early_stop],\n",
    "         epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(X_test, batch_size=512, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(results, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.57      0.65      1384\n",
      "           1       0.94      0.91      0.93      3838\n",
      "           2       0.87      0.97      0.92      3703\n",
      "\n",
      "    accuracy                           0.88      8925\n",
      "   macro avg       0.86      0.82      0.83      8925\n",
      "weighted avg       0.88      0.88      0.88      8925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try CNN with it this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_76\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_75 (Embedding)     (None, 34, 200)           6751200   \n",
      "_________________________________________________________________\n",
      "conv1d_107 (Conv1D)          (None, 34, 256)           256256    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_104 (MaxPoolin (None, 11, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_108 (Conv1D)          (None, 11, 128)           163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_105 (MaxPoolin (None, 3, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_109 (Conv1D)          (None, 3, 64)             41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_106 (MaxPoolin (None, 1, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_196 (Dense)            (None, 256)               16640     \n",
      "_________________________________________________________________\n",
      "dropout_141 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_142 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_198 (Dense)            (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_143 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_199 (Dense)            (None, 3)                 771       \n",
      "=================================================================\n",
      "Total params: 7,361,443\n",
      "Trainable params: 7,361,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(VOCAB_SIZE, EMBED_SIZE,\n",
    "                                    weights=[feature_embed],\n",
    "                                    trainable=True,\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "model.add(Conv1D(filters=256, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "\n",
    "model.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "\n",
    "model.add(Conv1D(filters=64, kernel_size=5, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=3))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                             patience=3,\n",
    "                                             restore_best_weights=True,\n",
    "                                             verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 35698 samples, validate on 8925 samples\n",
      "Epoch 1/100\n",
      "35698/35698 [==============================] - 14s 396us/sample - loss: 0.5272 - categorical_accuracy: 0.7843 - val_loss: 0.3283 - val_categorical_accuracy: 0.8728\n",
      "Epoch 2/100\n",
      "35698/35698 [==============================] - 14s 379us/sample - loss: 0.2909 - categorical_accuracy: 0.8896 - val_loss: 0.2964 - val_categorical_accuracy: 0.8878\n",
      "Epoch 3/100\n",
      "35698/35698 [==============================] - 14s 383us/sample - loss: 0.1840 - categorical_accuracy: 0.9296 - val_loss: 0.2946 - val_categorical_accuracy: 0.8918\n",
      "Epoch 4/100\n",
      "35698/35698 [==============================] - 14s 381us/sample - loss: 0.1064 - categorical_accuracy: 0.9627 - val_loss: 0.3513 - val_categorical_accuracy: 0.8893\n",
      "Epoch 5/100\n",
      "35698/35698 [==============================] - 13s 377us/sample - loss: 0.0623 - categorical_accuracy: 0.9791 - val_loss: 0.4397 - val_categorical_accuracy: 0.8877\n",
      "Epoch 6/100\n",
      "35584/35698 [============================>.] - ETA: 0s - loss: 0.0416 - categorical_accuracy: 0.9870Restoring model weights from the end of the best epoch.\n",
      "35698/35698 [==============================] - 14s 390us/sample - loss: 0.0415 - categorical_accuracy: 0.9870 - val_loss: 0.4986 - val_categorical_accuracy: 0.9036\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f297148a668>"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train_hot, validation_data=[X_test, y_test_hot], callbacks=[early_stop],\n",
    "         epochs=EPOCHS, batch_size=BATCH_SIZE, shuffle=True, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.predict(X_test, batch_size=512, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(results, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.63      0.69      1384\n",
      "           1       0.92      0.93      0.93      3838\n",
      "           2       0.90      0.95      0.92      3703\n",
      "\n",
      "    accuracy                           0.89      8925\n",
      "   macro avg       0.86      0.84      0.85      8925\n",
      "weighted avg       0.89      0.89      0.89      8925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
